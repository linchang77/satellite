# Prometheusç›‘æ§æ•°æ®-ç»™é™ˆå›è°ƒç”¨çš„æ–¹æ³•å’Œæ•°æ®





------

## ä¸€ã€èŠ‚ç‚¹ CPU è¿‘ 7 å¤©æ•°æ®å¯¼å‡º



```

curl -s "http://10.110.227.33:9090/api/v1/query_range" \
--data-urlencode "query=100 - (avg by (instance) (irate(node_cpu_seconds_total{mode='idle'}[5m])) * 100)" \
--data-urlencode "start=$(date -d '-7 day' +%s)" \
--data-urlencode "end=$(date +%s)" \
--data-urlencode "step=1h" | jq -r '.data.result[] | .metric as $metric | .values[] | [$metric.instance, .[0], .[1]] | @csv' > node_cpu_7d.csv
```

### æ‰§è¡ŒåéªŒè¯ï¼š



```
# æŸ¥çœ‹CSVå†…å®¹ï¼ˆéç©ºå³æˆåŠŸï¼‰
cat node_cpu_7d.csv
# æŸ¥çœ‹æ–‡ä»¶å¤§å°
ls -hl node_cpu_7d.csv
```

é¢„æœŸè¾“å‡ºï¼š

è¿™é‡Œåªæœ‰æ•°æ®ï¼Œæ²¡æœ‰è¡¨å¤´ï¼Œæ—¶é—´æˆ³ä¹Ÿä¸æ˜¯å¯è¯»æ ¼å¼

```
"10.2.4.10:9100",1768965271.189,"1.533333333403192"
"10.1.1.10:9100",1768965271.189,"0.8625000001241716"
```

### æ‰‹åŠ¨è½¬æ¢æ—¶é—´æˆ³ä¸ºå¯è¯»æ ¼å¼ï¼ˆå¯¼å‡ºåå¤„ç†ï¼‰

å¦‚æœéœ€è¦å¯è¯»æ—¶é—´ï¼Œå¯åœ¨å¯¼å‡º CSV åç”¨ä»¥ä¸‹å‘½ä»¤è½¬æ¢ï¼š

```
# è½¬æ¢CSVä¸­çš„Unixæ—¶é—´æˆ³ä¸ºå¯è¯»æ—¶é—´
awk -F ',' '{cmd="date -d @" $2 " +%Y-%m-%d_%H:%M:%S"; cmd | getline dt; close(cmd); $2=dt; print}' OFS=',' node_cpu_7d.csv > node_cpu_7d_with_time.csv
```



------

## äºŒã€å…¶ä»–èµ„æºæŸ¥è¯¢å‘½ä»¤

### 1. èŠ‚ç‚¹å†…å­˜ä½¿ç”¨ç‡ï¼ˆè¿‘ 7 å¤©ï¼‰



```
curl -s "http://10.110.227.33:9090/api/v1/query_range" \
--data-urlencode "query=(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100" \
--data-urlencode "start=$(date -d '-7 day' +%s)" \
--data-urlencode "end=$(date +%s)" \
--data-urlencode "step=1h" | jq -r '.data.result[] | .metric as $metric | .values[] | [$metric.instance, .[0], .[1]] | @csv' > node_mem_7d.csv
```

### 2. èŠ‚ç‚¹ç½‘ç»œå…¥æµé‡ï¼ˆè¿‘ 7 å¤©ï¼‰



```
curl -s "http://10.110.227.33:9090/api/v1/query_range" \
--data-urlencode "query=rate(node_network_receive_bytes_total[5m])" \
--data-urlencode "start=$(date -d '-7 day' +%s)" \
--data-urlencode "end=$(date +%s)" \
--data-urlencode "step=1h" | jq -r '.data.result[] | .metric as $metric | .values[] | [$metric.instance, $metric.device, .[0], .[1]] | @csv' > node_network_in_7d.csv
```

### 3. èŠ‚ç‚¹ç½‘ç»œå‡ºæµé‡ï¼ˆè¿‘ 7 å¤©ï¼‰



```
curl -s "http://10.110.227.33:9090/api/v1/query_range" \
--data-urlencode "query=rate(node_network_transmit_bytes_total[5m])" \
--data-urlencode "start=$(date -d '-7 day' +%s)" \
--data-urlencode "end=$(date +%s)" \
--data-urlencode "step=1h" | jq -r '.data.result[] | .metric as $metric | .values[] | [$metric.instance, $metric.device, .[0], .[1]] | @csv' > node_network_out_7d.csv
```

### 4. Pod CPU ä½¿ç”¨ç‡ï¼ˆè¿‘ 7 å¤©ï¼‰



```
curl -s "http://10.110.227.33:9090/api/v1/query_range" \
--data-urlencode "query=rate(container_cpu_usage_seconds_total{container!='POD',image!=''}[5m])" \
--data-urlencode "start=$(date -d '-7 day' +%s)" \
--data-urlencode "end=$(date +%s)" \
--data-urlencode "step=1h" | jq -r '.data.result[] | .metric as $metric | .values[] | [$metric.namespace, $metric.pod, $metric.container, .[0], .[1]] | @csv' > pod_cpu_7d.csv
```

### 5. Pod å†…å­˜ä½¿ç”¨é‡ï¼ˆè¿‘ 7 å¤©ï¼‰



```
curl -s "http://10.110.227.33:9090/api/v1/query_range" \
--data-urlencode "query=container_memory_usage_bytes{container!='POD',image!=''}" \
--data-urlencode "start=$(date -d '-7 day' +%s)" \
--data-urlencode "end=$(date +%s)" \
--data-urlencode "step=1h" | jq -r '.data.result[] | .metric as $metric | .values[] | [$metric.namespace, $metric.pod, $metric.container, .[0], .[1]] | @csv' > pod_mem_7d.csv
```

### 6. Pod ç½‘ç»œå…¥ / å‡ºæµé‡ï¼ˆè¿‘ 7 å¤©ï¼‰



```
# Podç½‘ç»œå…¥æµé‡
curl -s "http://10.110.227.33:9090/api/v1/query_range" \
--data-urlencode "query=rate(container_network_receive_bytes_total{container!='POD',image!=''}[5m])" \
--data-urlencode "start=$(date -d '-7 day' +%s)" \
--data-urlencode "end=$(date +%s)" \
--data-urlencode "step=1h" | jq -r '.data.result[] | .metric as $metric | .values[] | [$metric.namespace, $metric.pod, $metric.container, .[0], .[1]] | @csv' > pod_network_in_7d.csv

# Podç½‘ç»œå‡ºæµé‡
curl -s "http://10.110.227.33:9090/api/v1/query_range" \
--data-urlencode "query=rate(container_network_transmit_bytes_total{container!='POD',image!=''}[5m])" \
--data-urlencode "start=$(date -d '-7 day' +%s)" \
--data-urlencode "end=$(date +%s)" \
--data-urlencode "step=1h" | jq -r '.data.result[] | .metric as $metric | .values[] | [$metric.namespace, $metric.pod, $metric.container, .[0], .[1]] | @csv' > pod_network_out_7d.csv
```

------





# II.ç‰ˆæœ¬2-åŠ äº†å­—æ®µå’Œå•ä½

å¯¼å‡ºå¸¦è¡¨å¤´ + å•ä½çš„éç©º CSVï¼š

------

## ä¸€ã€èŠ‚ç‚¹ CPU è¿‘ 7 å¤©ï¼ˆå¸¦è¡¨å¤´ + å•ä½ï¼Œæ— æŠ¥é”™ï¼‰



```
# æ ¸å¿ƒä¿®æ­£ï¼šè¡¨å¤´ç”¨echoå•ç‹¬è¾“å‡ºï¼Œæ•°æ®è¡Œç”¨jqè§£æï¼ˆåˆ†å¼€å¤„ç†é¿å…@csvæ ¼å¼åŒ–å†²çªï¼‰
# æ­¥éª¤1ï¼šè¾“å‡ºè¡¨å¤´åˆ°CSV
echo "èŠ‚ç‚¹IP,æ—¶é—´æˆ³,CPUä½¿ç”¨ç‡(%)" > node_cpu_7d.csv
# æ­¥éª¤2ï¼šè¿½åŠ æ•°æ®è¡Œï¼ˆæ‹¼æ¥%å•ä½ï¼‰åˆ°CSV
curl -s "http://10.110.227.33:9090/api/v1/query_range" \
--data-urlencode "query=100 - (avg by (instance) (irate(node_cpu_seconds_total{mode='idle'}[5m])) * 100)" \
--data-urlencode "start=$(date -d '-7 day' +%s)" \
--data-urlencode "end=$(date +%s)" \
--data-urlencode "step=1h" | jq -r '.data.result[] | .metric as $metric | .values[] | [$metric.instance, .[0], (.[1] + "%")] | @csv' >> node_cpu_7d.csv
```

### æ‰§è¡ŒåéªŒè¯ï¼š

```
# æŸ¥çœ‹CSVå†…å®¹ï¼ˆè¡¨å¤´+æ•°æ®è¡Œï¼‰
cat node_cpu_7d.csv
# æŸ¥çœ‹æ–‡ä»¶å¤§å°
ls -hl node_cpu_7d.csv
```

é¢„æœŸè¾“å‡ºï¼š

```
èŠ‚ç‚¹IP,æ—¶é—´æˆ³,CPUä½¿ç”¨ç‡(%)
"10.2.4.10:9100",1768965271.189,"1.533333333403192%"
"10.1.1.10:9100",1768965271.189,"0.8625000001241716%"
```

------

## äºŒã€å…¶ä»–èµ„æºçš„æœ€ç»ˆå‘½ä»¤ï¼ˆå¸¦è¡¨å¤´ + å•ä½ï¼‰

### 1. èŠ‚ç‚¹å†…å­˜ä½¿ç”¨ç‡ï¼ˆè¡¨å¤´ï¼šèŠ‚ç‚¹ IP, æ—¶é—´æˆ³ï¼Œå†…å­˜ä½¿ç”¨ç‡ (%)ï¼‰

```
# è¾“å‡ºè¡¨å¤´
echo "èŠ‚ç‚¹IP,æ—¶é—´æˆ³,å†…å­˜ä½¿ç”¨ç‡(%)" > node_mem_7d.csv
# è¿½åŠ æ•°æ®è¡Œ
curl -s "http://10.110.227.33:9090/api/v1/query_range" \
--data-urlencode "query=(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100" \
--data-urlencode "start=$(date -d '-7 day' +%s)" \
--data-urlencode "end=$(date +%s)" \
--data-urlencode "step=1h" | jq -r '.data.result[] | .metric as $metric | .values[] | [$metric.instance, .[0], (.[1] + "%")] | @csv' >> node_mem_7d.csv
```

### 2. èŠ‚ç‚¹ç½‘ç»œå…¥æµé‡é€Ÿç‡ï¼ˆè¡¨å¤´ï¼šèŠ‚ç‚¹ IP, ç½‘å¡ï¼Œæ—¶é—´æˆ³ï¼Œå…¥æµé‡é€Ÿç‡ (B/s)ï¼‰

```
# è¾“å‡ºè¡¨å¤´
echo "èŠ‚ç‚¹IP,ç½‘å¡,æ—¶é—´æˆ³,å…¥æµé‡é€Ÿç‡(B/s)" > node_network_in_7d.csv
# è¿½åŠ æ•°æ®è¡Œ
curl -s "http://10.110.227.33:9090/api/v1/query_range" \
--data-urlencode "query=rate(node_network_receive_bytes_total[5m])" \
--data-urlencode "start=$(date -d '-7 day' +%s)" \
--data-urlencode "end=$(date +%s)" \
--data-urlencode "step=1h" | jq -r '.data.result[] | .metric as $metric | .values[] | [$metric.instance, $metric.device, .[0], (.[1] + " B/s")] | @csv' >> node_network_in_7d.csv
```

### 3. èŠ‚ç‚¹ç½‘ç»œå‡ºæµé‡é€Ÿç‡ï¼ˆè¡¨å¤´ï¼šèŠ‚ç‚¹ IP, ç½‘å¡ï¼Œæ—¶é—´æˆ³ï¼Œå‡ºæµé‡é€Ÿç‡ (B/s)ï¼‰



```
# è¾“å‡ºè¡¨å¤´
echo "èŠ‚ç‚¹IP,ç½‘å¡,æ—¶é—´æˆ³,å‡ºæµé‡é€Ÿç‡(B/s)" > node_network_out_7d.csv
# è¿½åŠ æ•°æ®è¡Œ
curl -s "http://10.110.227.33:9090/api/v1/query_range" \
--data-urlencode "query=rate(node_network_transmit_bytes_total[5m])" \
--data-urlencode "start=$(date -d '-7 day' +%s)" \
--data-urlencode "end=$(date +%s)" \
--data-urlencode "step=1h" | jq -r '.data.result[] | .metric as $metric | .values[] | [$metric.instance, $metric.device, .[0], (.[1] + " B/s")] | @csv' >> node_network_out_7d.csv
```

### 4. Pod CPU ä½¿ç”¨ç‡ï¼ˆè¡¨å¤´ï¼šå‘½åç©ºé—´ï¼ŒPod åç§°ï¼Œå®¹å™¨åç§°ï¼Œæ—¶é—´æˆ³ï¼ŒCPU ä½¿ç”¨ç‡ (æ ¸)ï¼‰

```
# è¾“å‡ºè¡¨å¤´
echo "å‘½åç©ºé—´,Podåç§°,å®¹å™¨åç§°,æ—¶é—´æˆ³,CPUä½¿ç”¨ç‡(æ ¸)" > pod_cpu_7d.csv
# è¿½åŠ æ•°æ®è¡Œ
curl -s "http://10.110.227.33:9090/api/v1/query_range" \
--data-urlencode "query=rate(container_cpu_usage_seconds_total{container!='POD',image!=''}[5m])" \
--data-urlencode "start=$(date -d '-7 day' +%s)" \
--data-urlencode "end=$(date +%s)" \
--data-urlencode "step=1h" | jq -r '.data.result[] | .metric as $metric | .values[] | [$metric.namespace, $metric.pod, $metric.container, .[0], (.[1] + " æ ¸")] | @csv' >> pod_cpu_7d.csv
```

### 5. Pod å†…å­˜ä½¿ç”¨é‡ï¼ˆè¡¨å¤´ï¼šå‘½åç©ºé—´ï¼ŒPod åç§°ï¼Œå®¹å™¨åç§°ï¼Œæ—¶é—´æˆ³ï¼Œå†…å­˜ä½¿ç”¨é‡ (å­—èŠ‚)ï¼‰



```
# è¾“å‡ºè¡¨å¤´
echo "å‘½åç©ºé—´,Podåç§°,å®¹å™¨åç§°,æ—¶é—´æˆ³,å†…å­˜ä½¿ç”¨é‡(å­—èŠ‚)" > pod_mem_7d.csv
# è¿½åŠ æ•°æ®è¡Œ
curl -s "http://10.110.227.33:9090/api/v1/query_range" \
--data-urlencode "query=container_memory_usage_bytes{container!='POD',image!=''}" \
--data-urlencode "start=$(date -d '-7 day' +%s)" \
--data-urlencode "end=$(date +%s)" \
--data-urlencode "step=1h" | jq -r '.data.result[] | .metric as $metric | .values[] | [$metric.namespace, $metric.pod, $metric.container, .[0], (.[1] + " å­—èŠ‚")] | @csv' >> pod_mem_7d.csv
```

### 6. Pod ç½‘ç»œå…¥ / å‡ºæµé‡é€Ÿç‡



```
# Podç½‘ç»œå…¥æµé‡ï¼ˆè¡¨å¤´ï¼šå‘½åç©ºé—´,Podåç§°,å®¹å™¨åç§°,æ—¶é—´æˆ³,å…¥æµé‡é€Ÿç‡(B/s)ï¼‰
echo "å‘½åç©ºé—´,Podåç§°,å®¹å™¨åç§°,æ—¶é—´æˆ³,å…¥æµé‡é€Ÿç‡(B/s)" > pod_network_in_7d.csv
curl -s "http://10.110.227.33:9090/api/v1/query_range" \
--data-urlencode "query=rate(container_network_receive_bytes_total{container!='POD',image!=''}[5m])" \
--data-urlencode "start=$(date -d '-7 day' +%s)" \
--data-urlencode "end=$(date +%s)" \
--data-urlencode "step=1h" | jq -r '.data.result[] | .metric as $metric | .values[] | [$metric.namespace, $metric.pod, $metric.container, .[0], (.[1] + " B/s")] | @csv' >> pod_network_in_7d.csv

# Podç½‘ç»œå‡ºæµé‡ï¼ˆè¡¨å¤´ï¼šå‘½åç©ºé—´,Podåç§°,å®¹å™¨åç§°,æ—¶é—´æˆ³,å‡ºæµé‡é€Ÿç‡(B/s)ï¼‰
echo "å‘½åç©ºé—´,Podåç§°,å®¹å™¨åç§°,æ—¶é—´æˆ³,å‡ºæµé‡é€Ÿç‡(B/s)" > pod_network_out_7d.csv
curl -s "http://10.110.227.33:9090/api/v1/query_range" \
--data-urlencode "query=rate(container_network_transmit_bytes_total{container!='POD',image!=''}[5m])" \
--data-urlencode "start=$(date -d '-7 day' +%s)" \
--data-urlencode "end=$(date +%s)" \
--data-urlencode "step=1h" | jq -r '.data.result[] | .metric as $metric | .values[] | [$metric.namespace, $metric.pod, $metric.container, .[0], (.[1] + " B/s")] | @csv' >> pod_network_out_7d.csv
```

------







# III.K8s ç›‘æ§æ•°æ®å¯¼å‡º Python è„šæœ¬



**ä¿®å¤ç‰ˆè„šæœ¬ï¼š`export_k8s_metrics_final.py`**

```
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
K8sé›†ç¾¤è¿‘7å¤©èµ„æºç›‘æ§æ•°æ®å¯¼å‡ºè„šæœ¬ï¼ˆç»ˆæä¿®å¤ç‰ˆï¼‰
æ ¸å¿ƒæ”¹è¿›ï¼š
1. è”åŠ¨K8s APIè·å–é›†ç¾¤æ‰€æœ‰Pod-å®¹å™¨æ˜ å°„å…³ç³»ï¼Œç²¾å‡†åŒ¹é…å®¹å™¨åç§°
2. ä¸å†ä¾èµ–Prometheusçš„å®¹å™¨æ ‡ç­¾ï¼Œå½»åº•è§£å†³æ ‡ç­¾ç¼ºå¤±é—®é¢˜
3. æ”¯æŒç¦»çº¿ç¼“å­˜å®¹å™¨æ˜ å°„å…³ç³»ï¼Œæå‡æ‰§è¡Œæ•ˆç‡
4. å…¼å®¹æ‰€æœ‰K8sç»„ä»¶ï¼ˆnode-exporter/coredns/kube-proxyç­‰ï¼‰
"""

import requests
import json
import os
import time
from datetime import datetime
from pathlib import Path
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# ===================== é…ç½®é¡¹ï¼ˆå¯æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰=====================
# æ–°é…ç½®ï¼ˆmasterèŠ‚ç‚¹NodePortï¼‰
#PROMETHEUS_URL = "http://10.2.3.10:30090/api/v1/query_range"  # å…³é”®ä¿®æ”¹

PROMETHEUS_URL = "http://10.110.227.33:9090/api/v1/query_range"  # Prometheusåœ°å€
EXPORT_DIR = f"k8s_metrics_{datetime.now().strftime('%Y%m%d')}"   # å¯¼å‡ºç›®å½•ï¼ˆæŒ‰æ—¥æœŸå‘½åï¼‰
TIME_RANGE_DAYS = 7                                              # æ•°æ®æ—¶é—´èŒƒå›´ï¼ˆè¿‘7å¤©ï¼‰
STEP = "1h"                                                      # é‡‡æ ·æ­¥é•¿ï¼ˆ1å°æ—¶1ä¸ªç‚¹ï¼‰
CACHE_CONTAINER_MAP = True                                       # ç¼“å­˜å®¹å™¨æ˜ å°„å…³ç³»ï¼ˆé¿å…é‡å¤æŸ¥è¯¢K8s APIï¼‰
CACHE_FILE = "./k8s_container_map.json"                          # å®¹å™¨æ˜ å°„ç¼“å­˜æ–‡ä»¶

# ===================== å…¨å±€å˜é‡ =====================
# Pod-å®¹å™¨æ˜ å°„å­—å…¸ï¼škey=(namespace, pod_name) â†’ value=å®¹å™¨åç§°åˆ—è¡¨
CONTAINER_MAP = {}

# ===================== å·¥å…·å‡½æ•° =====================
def create_export_dir():
    """åˆ›å»ºå¯¼å‡ºç›®å½•"""
    Path(EXPORT_DIR).mkdir(exist_ok=True)
    print(f"âœ… åˆ›å»ºå¯¼å‡ºç›®å½•ï¼š{EXPORT_DIR}")

def load_k8s_config():
    """åŠ è½½K8sé…ç½®ï¼ˆå…¼å®¹é›†ç¾¤å†…/é›†ç¾¤å¤–æ‰§è¡Œï¼‰"""
    try:
        # ä¼˜å…ˆåŠ è½½é›†ç¾¤å†…é…ç½®ï¼ˆPodå†…æ‰§è¡Œï¼‰
        config.load_incluster_config()
        print("âœ… åŠ è½½K8sé›†ç¾¤å†…é…ç½®æˆåŠŸ")
    except config.ConfigException:
        # åŠ è½½æœ¬åœ°kubeconfigï¼ˆå¼€å‘æœºæ‰§è¡Œï¼‰
        config.load_kube_config()
        print("âœ… åŠ è½½æœ¬åœ°K8sé…ç½®æˆåŠŸ")

def get_k8s_container_map():
    """
    ä»K8s APIè·å–æ‰€æœ‰Pod-å®¹å™¨æ˜ å°„å…³ç³»
    è¿”å›æ ¼å¼ï¼š{(namespace, pod_name): [container1, container2, ...], ...}
    """
    global CONTAINER_MAP
    
    # ä¼˜å…ˆè¯»å–ç¼“å­˜
    if CACHE_CONTAINER_MAP and os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                CONTAINER_MAP = json.load(f)
                print(f"âœ… ä»ç¼“å­˜åŠ è½½å®¹å™¨æ˜ å°„å…³ç³»ï¼ˆå…±{len(CONTAINER_MAP)}ä¸ªPodï¼‰")
                return CONTAINER_MAP
        except Exception as e:
            print(f"âš ï¸  è¯»å–ç¼“å­˜å¤±è´¥ï¼Œé‡æ–°æŸ¥è¯¢K8s APIï¼š{str(e)}")
    
    # ä»K8s APIæŸ¥è¯¢
    load_k8s_config()
    v1 = client.CoreV1Api()
    container_map = {}
    
    try:
        # æŸ¥è¯¢æ‰€æœ‰å‘½åç©ºé—´çš„Pod
        ret = v1.list_pod_for_all_namespaces(watch=False)
        for pod in ret.items:
            ns = pod.metadata.namespace
            pod_name = pod.metadata.name
            # è·å–Podçš„æ‰€æœ‰å®¹å™¨åç§°
            containers = [c.name for c in pod.spec.containers]
            container_map[(ns, pod_name)] = containers
        
        CONTAINER_MAP = container_map
        print(f"âœ… ä»K8s APIè·å–å®¹å™¨æ˜ å°„å…³ç³»ï¼ˆå…±{len(container_map)}ä¸ªPodï¼‰")
        
        # å†™å…¥ç¼“å­˜
        if CACHE_CONTAINER_MAP:
            with open(CACHE_FILE, "w", encoding="utf-8") as f:
                json.dump(container_map, f, ensure_ascii=False, indent=2)
            print(f"âœ… å®¹å™¨æ˜ å°„å…³ç³»å·²ç¼“å­˜åˆ°ï¼š{CACHE_FILE}")
        
        return container_map
    
    except ApiException as e:
        print(f"âŒ æŸ¥è¯¢K8s APIå¤±è´¥ï¼š{e.reason}ï¼ˆ{e.status}ï¼‰")
        return {}
    except Exception as e:
        print(f"âŒ è·å–å®¹å™¨æ˜ å°„å…³ç³»å¤±è´¥ï¼š{str(e)}")
        return {}

def get_container_name_from_k8s(namespace: str, pod_name: str) -> str:
    """
    ä»K8sæ˜ å°„å…³ç³»ä¸­è·å–å®¹å™¨åç§°ï¼ˆé€‚é…å•å®¹å™¨Podï¼‰
    """
    # å¤„ç†Podåç§°åç¼€ï¼ˆå¦‚xxx-mgcfj â†’ xxxï¼‰
    pod_name_base = pod_name.rsplit("-", 1)[0] if "-" in pod_name else pod_name
    
    # ç²¾å‡†åŒ¹é…ï¼ˆnamespace + pod_nameï¼‰
    key_exact = (namespace, pod_name)
    if key_exact in CONTAINER_MAP:
        containers = CONTAINER_MAP[key_exact]
        # å•å®¹å™¨Podç›´æ¥è¿”å›å®¹å™¨å
        if len(containers) == 1:
            return containers[0]
        # å¤šå®¹å™¨Podè¿”å›é€—å·åˆ†éš”
        return ", ".join(containers)
    
    # æ¨¡ç³ŠåŒ¹é…ï¼ˆå¤„ç†Prometheusä¸­Podåç§°ç¼ºå¤±åç¼€çš„æƒ…å†µï¼‰
    for (ns, pn), containers in CONTAINER_MAP.items():
        if ns == namespace and (pn == pod_name_base or pod_name in pn):
            if len(containers) == 1:
                return containers[0]
            return ", ".join(containers)
    
    # æœ€ç»ˆé™çº§
    return f"{pod_name}-æœªçŸ¥å®¹å™¨"

def get_prometheus_data(query: str) -> list:
    """ä»Prometheusè·å–æ•°æ®"""
    end_ts = int(time.time())
    start_ts = end_ts - (TIME_RANGE_DAYS * 24 * 3600)
    
    params = {
        "query": query,
        "start": start_ts,
        "end": end_ts,
        "step": STEP
    }
    
    try:
        response = requests.get(PROMETHEUS_URL, params=params, timeout=60)
        response.raise_for_status()
        data = response.json()
        
        if data.get("status") != "success":
            print(f"âŒ PrometheusæŸ¥è¯¢å¤±è´¥ï¼š{data.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return []
        
        return data.get("data", {}).get("result", [])
    
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚Prometheuså¤±è´¥ï¼š{str(e)}")
        return []

def timestamp_to_readable(ts: str) -> str:
    """æ—¶é—´æˆ³è½¬å¯è¯»æ ¼å¼"""
    try:
        ts_int = int(float(ts))
        return datetime.fromtimestamp(ts_int).strftime("%Y-%m-%d %H:%M:%S")
    except (ValueError, TypeError):
        return "æ—¶é—´è½¬æ¢å¤±è´¥"

def export_to_csv(file_path: str, headers: list, data_rows: list):
    """å¯¼å‡ºæ•°æ®åˆ°CSV"""
    try:
        import csv
        with open(file_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(headers)
            writer.writerows(data_rows)
        print(f"âœ… å¯¼å‡ºå®Œæˆï¼š{file_path}")
    except Exception as e:
        print(f"âŒ å¯¼å‡ºCSVå¤±è´¥ {file_path}ï¼š{str(e)}")

# ===================== æ•°æ®å¯¼å‡ºå‡½æ•° =====================
def export_node_cpu():
    """å¯¼å‡ºèŠ‚ç‚¹CPUä½¿ç”¨ç‡"""
    print("\nğŸ”„ å¼€å§‹å¯¼å‡ºèŠ‚ç‚¹CPUä½¿ç”¨ç‡...")
    query = "100 - (avg by (instance) (irate(node_cpu_seconds_total{mode='idle'}[5m])) * 100)"
    data = get_prometheus_data(query)
    
    headers = [
        "Node IP(èŠ‚ç‚¹IP)",
        "Time(æ—¶é—´)(YYYY-MM-DD HH:MM:SS)",
        "CPU Usage(CPUä½¿ç”¨ç‡)(%)"
    ]
    
    data_rows = []
    for item in data:
        instance = item["metric"].get("instance", "æœªçŸ¥èŠ‚ç‚¹")
        for value_pair in item["values"]:
            ts, cpu_value = value_pair
            readable_time = timestamp_to_readable(ts)
            data_rows.append([instance, readable_time, f"{cpu_value}%"])
    
    export_to_csv(f"{EXPORT_DIR}/node_cpu_7d.csv", headers, data_rows)

def export_node_mem():
    """å¯¼å‡ºèŠ‚ç‚¹å†…å­˜ä½¿ç”¨ç‡"""
    print("\nğŸ”„ å¼€å§‹å¯¼å‡ºèŠ‚ç‚¹å†…å­˜ä½¿ç”¨ç‡...")
    query = "(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100"
    data = get_prometheus_data(query)
    
    headers = [
        "Node IP(èŠ‚ç‚¹IP)",
        "Time(æ—¶é—´)(YYYY-MM-DD HH:MM:SS)",
        "Memory Usage(å†…å­˜ä½¿ç”¨ç‡)(%)"
    ]
    
    data_rows = []
    for item in data:
        instance = item["metric"].get("instance", "æœªçŸ¥èŠ‚ç‚¹")
        for value_pair in item["values"]:
            ts, mem_value = value_pair
            readable_time = timestamp_to_readable(ts)
            data_rows.append([instance, readable_time, f"{mem_value}%"])
    
    export_to_csv(f"{EXPORT_DIR}/node_mem_7d.csv", headers, data_rows)

def export_node_network_in():
    """å¯¼å‡ºèŠ‚ç‚¹ç½‘ç»œå…¥æµé‡é€Ÿç‡"""
    print("\nğŸ”„ å¼€å§‹å¯¼å‡ºèŠ‚ç‚¹ç½‘ç»œå…¥æµé‡é€Ÿç‡...")
    query = "rate(node_network_receive_bytes_total[5m])"
    data = get_prometheus_data(query)
    
    headers = [
        "Node IP(èŠ‚ç‚¹IP)",
        "Network Card(ç½‘å¡)",
        "Time(æ—¶é—´)(YYYY-MM-DD HH:MM:SS)",
        "Inbound Traffic Rate(å…¥æµé‡é€Ÿç‡)(B/s)"
    ]
    
    data_rows = []
    for item in data:
        instance = item["metric"].get("instance", "æœªçŸ¥èŠ‚ç‚¹")
        device = item["metric"].get("device", "æœªçŸ¥ç½‘å¡")
        for value_pair in item["values"]:
            ts, in_value = value_pair
            readable_time = timestamp_to_readable(ts)
            data_rows.append([instance, device, readable_time, f"{in_value} B/s"])
    
    export_to_csv(f"{EXPORT_DIR}/node_network_in_7d.csv", headers, data_rows)

def export_node_network_out():
    """å¯¼å‡ºèŠ‚ç‚¹ç½‘ç»œå‡ºæµé‡é€Ÿç‡"""
    print("\nğŸ”„ å¼€å§‹å¯¼å‡ºèŠ‚ç‚¹ç½‘ç»œå‡ºæµé‡é€Ÿç‡...")
    query = "rate(node_network_transmit_bytes_total[5m])"
    data = get_prometheus_data(query)
    
    headers = [
        "Node IP(èŠ‚ç‚¹IP)",
        "Network Card(ç½‘å¡)",
        "Time(æ—¶é—´)(YYYY-MM-DD HH:MM:SS)",
        "Outbound Traffic Rate(å‡ºæµé‡é€Ÿç‡)(B/s)"
    ]
    
    data_rows = []
    for item in data:
        instance = item["metric"].get("instance", "æœªçŸ¥èŠ‚ç‚¹")
        device = item["metric"].get("device", "æœªçŸ¥ç½‘å¡")
        for value_pair in item["values"]:
            ts, out_value = value_pair
            readable_time = timestamp_to_readable(ts)
            data_rows.append([instance, device, readable_time, f"{out_value} B/s"])
    
    export_to_csv(f"{EXPORT_DIR}/node_network_out_7d.csv", headers, data_rows)

def export_pod_cpu():
    """å¯¼å‡ºPod CPUä½¿ç”¨ç‡ï¼ˆä»K8s APIè·å–å®¹å™¨åç§°ï¼‰"""
    print("\nğŸ”„ å¼€å§‹å¯¼å‡ºPod CPUä½¿ç”¨ç‡...")
    query = "rate(container_cpu_usage_seconds_total{image!=''}[5m])"
    data = get_prometheus_data(query)
    
    headers = [
        "Namespace(å‘½åç©ºé—´)",
        "Pod Name(Podåç§°)",
        "Container Name(å®¹å™¨åç§°)",
        "Time(æ—¶é—´)(YYYY-MM-DD HH:MM:SS)",
        "CPU Usage(CPUä½¿ç”¨ç‡)(Core)"
    ]
    
    data_rows = []
    for item in data:
        namespace = item["metric"].get("namespace", "æœªçŸ¥å‘½åç©ºé—´")
        pod_name = item["metric"].get("pod", "æœªçŸ¥Pod")
        # ä»K8sæ˜ å°„å…³ç³»è·å–å®¹å™¨åç§°
        container_name = get_container_name_from_k8s(namespace, pod_name)
        
        for value_pair in item["values"]:
            ts, cpu_value = value_pair
            readable_time = timestamp_to_readable(ts)
            data_rows.append([namespace, pod_name, container_name, readable_time, f"{cpu_value} Core"])
    
    export_to_csv(f"{EXPORT_DIR}/pod_cpu_7d.csv", headers, data_rows)

def export_pod_mem():
    """å¯¼å‡ºPodå†…å­˜ä½¿ç”¨é‡ï¼ˆä»K8s APIè·å–å®¹å™¨åç§°ï¼‰"""
    print("\nğŸ”„ å¼€å§‹å¯¼å‡ºPodå†…å­˜ä½¿ç”¨é‡...")
    query = "container_memory_usage_bytes{image!=''}"
    data = get_prometheus_data(query)
    
    headers = [
        "Namespace(å‘½åç©ºé—´)",
        "Pod Name(Podåç§°)",
        "Container Name(å®¹å™¨åç§°)",
        "Time(æ—¶é—´)(YYYY-MM-DD HH:MM:SS)",
        "Memory Usage(å†…å­˜ä½¿ç”¨é‡)(GB)"  # æ”¹ä¸ºGBæ›´æ˜“è¯»
    ]
    
    data_rows = []
    for item in data:
        namespace = item["metric"].get("namespace", "æœªçŸ¥å‘½åç©ºé—´")
        pod_name = item["metric"].get("pod", "æœªçŸ¥Pod")
        # ä»K8sæ˜ å°„å…³ç³»è·å–å®¹å™¨åç§°
        container_name = get_container_name_from_k8s(namespace, pod_name)
        
        for value_pair in item["values"]:
            ts, mem_value = value_pair
            readable_time = timestamp_to_readable(ts)
            # è½¬æ¢ä¸ºGBï¼ˆä¿ç•™2ä½å°æ•°ï¼‰
            try:
                mem_gb = round(float(mem_value) / 1024 / 1024 / 1024, 2)
                mem_str = f"{mem_gb} GB"
            except ValueError:
                mem_str = f"{mem_value} Byte"
            data_rows.append([namespace, pod_name, container_name, readable_time, mem_str])
    
    export_to_csv(f"{EXPORT_DIR}/pod_mem_7d.csv", headers, data_rows)

def export_pod_network_in():
    """å¯¼å‡ºPodç½‘ç»œå…¥æµé‡é€Ÿç‡ï¼ˆä»K8s APIè·å–å®¹å™¨åç§°ï¼‰"""
    print("\nğŸ”„ å¼€å§‹å¯¼å‡ºPodç½‘ç»œå…¥æµé‡é€Ÿç‡...")
    query = "rate(container_network_receive_bytes_total{image!=''}[5m])"
    data = get_prometheus_data(query)
    
    headers = [
        "Namespace(å‘½åç©ºé—´)",
        "Pod Name(Podåç§°)",
        "Container Name(å®¹å™¨åç§°)",
        "Time(æ—¶é—´)(YYYY-MM-DD HH:MM:SS)",
        "Inbound Traffic Rate(å…¥æµé‡é€Ÿç‡)(B/s)"
    ]
    
    data_rows = []
    for item in data:
        namespace = item["metric"].get("namespace", "æœªçŸ¥å‘½åç©ºé—´")
        pod_name = item["metric"].get("pod", "æœªçŸ¥Pod")
        container_name = get_container_name_from_k8s(namespace, pod_name)
        
        for value_pair in item["values"]:
            ts, in_value = value_pair
            readable_time = timestamp_to_readable(ts)
            data_rows.append([namespace, pod_name, container_name, readable_time, f"{in_value} B/s"])
    
    export_to_csv(f"{EXPORT_DIR}/pod_network_in_7d.csv", headers, data_rows)

def export_pod_network_out():
    """å¯¼å‡ºPodç½‘ç»œå‡ºæµé‡é€Ÿç‡ï¼ˆä»K8s APIè·å–å®¹å™¨åç§°ï¼‰"""
    print("\nğŸ”„ å¼€å§‹å¯¼å‡ºPodç½‘ç»œå‡ºæµé‡é€Ÿç‡...")
    query = "rate(container_network_transmit_bytes_total{image!=''}[5m])"
    data = get_prometheus_data(query)
    
    headers = [
        "Namespace(å‘½åç©ºé—´)",
        "Pod Name(Podåç§°)",
        "Container Name(å®¹å™¨åç§°)",
        "Time(æ—¶é—´)(YYYY-MM-DD HH:MM:SS)",
        "Outbound Traffic Rate(å‡ºæµé‡é€Ÿç‡)(B/s)"
    ]
    
    data_rows = []
    for item in data:
        namespace = item["metric"].get("namespace", "æœªçŸ¥å‘½åç©ºé—´")
        pod_name = item["metric"].get("pod", "æœªçŸ¥Pod")
        container_name = get_container_name_from_k8s(namespace, pod_name)
        
        for value_pair in item["values"]:
            ts, out_value = value_pair
            readable_time = timestamp_to_readable(ts)
            data_rows.append([namespace, pod_name, container_name, readable_time, f"{out_value} B/s"])
    
    export_to_csv(f"{EXPORT_DIR}/pod_network_out_7d.csv", headers, data_rows)

# ===================== ä¸»å‡½æ•° =====================
def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¯¼å‡ºK8sé›†ç¾¤è¿‘{}å¤©ç›‘æ§æ•°æ®ï¼ˆç»ˆæä¿®å¤ç‰ˆï¼‰...".format(TIME_RANGE_DAYS))
    
    # 1. åˆ›å»ºå¯¼å‡ºç›®å½•
    create_export_dir()
    
    # 2. é¢„åŠ è½½K8så®¹å™¨æ˜ å°„å…³ç³»ï¼ˆæ ¸å¿ƒä¿®å¤ï¼‰
    print("\nğŸ”„ åŠ è½½K8så®¹å™¨æ˜ å°„å…³ç³»...")
    get_k8s_container_map()
    
    # 3. æ‰§è¡Œå¯¼å‡ºä»»åŠ¡
    export_node_cpu()
    export_node_mem()
    export_node_network_in()
    export_node_network_out()
    export_pod_cpu()
    export_pod_mem()
    export_pod_network_in()
    export_pod_network_out()
    
    # 4. è¾“å‡ºæ€»ç»“
    print("\n========================================")
    print("ğŸ‰ æ‰€æœ‰ç›‘æ§æ•°æ®å¯¼å‡ºå®Œæˆï¼")
    print(f"ğŸ“ å¯¼å‡ºç›®å½•ï¼š{os.path.abspath(EXPORT_DIR)}")
    print("ğŸ“‹ å¯¼å‡ºæ–‡ä»¶åˆ—è¡¨ï¼š")
    for file in os.listdir(EXPORT_DIR):
        print(f"   - {file}")
    print("========================================")

if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–
    required_pkgs = ["requests", "kubernetes"]
    missing_pkgs = []
    
    for pkg in required_pkgs:
        try:
            __import__(pkg)
        except ImportError:
            missing_pkgs.append(pkg)
    
    if missing_pkgs:
        print(f"âŒ ç¼ºå°‘ä¾èµ–åŒ…ï¼š{', '.join(missing_pkgs)}")
        print(f"ğŸ“¦ æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼špip3 install {' '.join(missing_pkgs)}")
        exit(1)
    
    # å¯åŠ¨ä¸»æµç¨‹
    main()
```



## äºŒã€ä½¿ç”¨æ­¥éª¤

### 1. å®‰è£…ä¾èµ–ï¼ˆæ–°å¢ kubernetes å®¢æˆ·ç«¯ï¼‰

```
pip3 install requests kubernetes
```

### 2. é…ç½® K8s è®¿é—®æƒé™

- **é›†ç¾¤å†…æ‰§è¡Œ**ï¼ˆå¦‚åœ¨ K8s çš„ Pod ä¸­è¿è¡Œï¼‰ï¼šæ— éœ€é¢å¤–é…ç½®ï¼Œè„šæœ¬è‡ªåŠ¨åŠ è½½ incluster é…ç½®ï¼›
- **æœ¬åœ°æ‰§è¡Œ**ï¼ˆå¼€å‘æœºï¼‰ï¼šç¡®ä¿`~/.kube/config`æ–‡ä»¶å­˜åœ¨ä¸”æœ‰æƒé™è®¿é—®é›†ç¾¤ã€‚

### 3. æ‰§è¡Œè„šæœ¬

```
python3 export_k8s_metrics_final.py
```



# IV.æœ€ç»ˆä¿®å¤ç‰ˆï¼šK8s ç›‘æ§æ•°æ®å¯¼å‡º Python è„šæœ¬

ä¿®æ”¹ç‚¹å¦‚ä¸‹ï¼šï¼ˆ1ï¼‰nodeçš„ç›‘æ§æ•°æ®ï¼Œç»™å‡ºæå–æ•°æ®å¯¹åº”çš„nodeåç§°å’Œnodeçš„IPã€‚ï¼ˆ2ï¼‰podçš„ç›‘æ§æ•°æ®ï¼Œç»™å‡ºæå–æ•°æ®å¯¹åº”çš„nodeåç§°å’Œnodeçš„Iï¼Œç»™å‡ºpodå¯¹åº”çš„IPåœ°å€ï¼Œå±äºå“ªä¸ªserviceï¼ŒContainer Nameçš„ä¸­æ–‡ç¿»è¯‘æ”¹ä¸ºâ€œå®¹å™¨å®ä¾‹åç§°â€ã€‚



```
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
K8sé›†ç¾¤è¿‘7å¤©èµ„æºç›‘æ§æ•°æ®å¯¼å‡ºè„šæœ¬ï¼ˆæœ€ç»ˆä¿®å¤ç‰ˆï¼‰
æ ¸å¿ƒç‰¹æ€§ï¼š
1. ä¿®å¤JSONå…ƒç»„KeyæŠ¥é”™é—®é¢˜ï¼ˆtuple â†’ string keyï¼‰
2. èŠ‚ç‚¹ç›‘æ§ï¼šåŒ…å«èŠ‚ç‚¹åç§°ã€èŠ‚ç‚¹IPç»´åº¦
3. Podç›‘æ§ï¼šåŒ…å«èŠ‚ç‚¹åç§°ã€èŠ‚ç‚¹IPã€Pod IPã€å…³è”Serviceç»´åº¦
4. å®¹å™¨åç§°ç¿»è¯‘ï¼š"å®¹å™¨å®ä¾‹åç§°"
5. è”åŠ¨K8s APIè·å–å…¨é‡å…ƒæ•°æ®ï¼Œæ•°æ®ç²¾å‡†æ— ç¼ºå¤±
"""

import requests
import json
import os
import time
from datetime import datetime
from pathlib import Path
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# ===================== é…ç½®é¡¹ï¼ˆå¯æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰=====================
# æ›¿æ¢ä¸ºä½ çš„Prometheusåœ°å€ï¼ˆNodePort/ClusterIPå‡å¯ï¼‰
PROMETHEUS_URL = "http://10.2.3.10:30090/api/v1/query_range"  
EXPORT_DIR = f"k8s_metrics_{datetime.now().strftime('%Y%m%d')}"   # å¯¼å‡ºç›®å½•
TIME_RANGE_DAYS = 7                                              # æ•°æ®æ—¶é—´èŒƒå›´ï¼ˆè¿‘7å¤©ï¼‰
STEP = "1h"                                                      # é‡‡æ ·æ­¥é•¿ï¼ˆ1å°æ—¶1ä¸ªç‚¹ï¼‰
CACHE_CONTAINER_MAP = True                                       # ç¼“å­˜å®¹å™¨æ˜ å°„å…³ç³»
CACHE_FILE = "./k8s_metadata_map.json"                           # å…ƒæ•°æ®ç¼“å­˜æ–‡ä»¶ï¼ˆå«èŠ‚ç‚¹/Pod/Serviceï¼‰

# ===================== å…¨å±€å˜é‡ =====================
# å…¨é‡å…ƒæ•°æ®æ˜ å°„ï¼šåŒ…å«èŠ‚ç‚¹/Pod/Serviceä¿¡æ¯
METADATA_MAP = {
    "node_map": {},          # èŠ‚ç‚¹å â†’ èŠ‚ç‚¹IP: {node_name: node_ip, ...}
    "pod_map": {},           # "ns|pod_name" â†’ {node_name, pod_ip, containers: [...]}
    "pod_service_map": {}    # "ns|pod_name" â†’ [service1, service2, ...]
}

# ===================== å·¥å…·å‡½æ•° =====================
def create_export_dir():
    """åˆ›å»ºå¯¼å‡ºç›®å½•"""
    Path(EXPORT_DIR).mkdir(exist_ok=True)
    print(f"âœ… åˆ›å»ºå¯¼å‡ºç›®å½•ï¼š{EXPORT_DIR}")

def load_k8s_config():
    """åŠ è½½K8sé…ç½®ï¼ˆå…¼å®¹é›†ç¾¤å†…/é›†ç¾¤å¤–æ‰§è¡Œï¼Œæ‰‹åŠ¨æŒ‡å®škubeconfigè·¯å¾„ï¼‰"""
    try:
        # ä¼˜å…ˆåŠ è½½é›†ç¾¤å†…é…ç½®ï¼ˆPodå†…æ‰§è¡Œï¼‰
        config.load_incluster_config()
        print("âœ… åŠ è½½K8sé›†ç¾¤å†…é…ç½®æˆåŠŸ")
    except config.ConfigException:
        # æ‰‹åŠ¨æŒ‡å®škubeconfigè·¯å¾„ï¼ˆè§£å†³èŠ‚ç‚¹å®¿ä¸»æœºæ‰§è¡Œçš„é…ç½®é—®é¢˜ï¼‰
        kubeconfig_path = "/etc/kubernetes/admin.conf"  # æ ¸å¿ƒä¿®æ”¹ï¼šæŒ‡å®škubeconfigè·¯å¾„
        if os.path.exists(kubeconfig_path):
            config.load_kube_config(config_file=kubeconfig_path)
            print(f"âœ… åŠ è½½æŒ‡å®šè·¯å¾„çš„K8sé…ç½®æˆåŠŸï¼š{kubeconfig_path}")
        else:
            # é™çº§åŠ è½½é»˜è®¤è·¯å¾„
            config.load_kube_config()
            print("âœ… åŠ è½½æœ¬åœ°K8sé…ç½®æˆåŠŸ")

def get_k8s_node_map():
    """è·å–èŠ‚ç‚¹åç§°â†’èŠ‚ç‚¹IPçš„æ˜ å°„"""
    v1 = client.CoreV1Api()
    node_map = {}
    try:
        nodes = v1.list_node(watch=False)
        for node in nodes.items:
            node_name = node.metadata.name
            # è·å–èŠ‚ç‚¹IPï¼ˆä¼˜å…ˆå–å†…ç½‘IPï¼‰
            for addr in node.status.addresses:
                if addr.type == "InternalIP":
                    node_map[node_name] = addr.address
                    break
        print(f"âœ… è·å–èŠ‚ç‚¹æ˜ å°„å…³ç³»ï¼ˆå…±{len(node_map)}ä¸ªèŠ‚ç‚¹ï¼‰")
        return node_map
    except ApiException as e:
        print(f"âŒ æŸ¥è¯¢èŠ‚ç‚¹ä¿¡æ¯å¤±è´¥ï¼š{e.reason}ï¼ˆ{e.status}ï¼‰")
        return {}

def get_k8s_pod_map():
    """è·å–Podå…ƒæ•°æ®æ˜ å°„ï¼š"ns|pod_name" â†’ {node_name, pod_ip, containers}"""
    v1 = client.CoreV1Api()
    pod_map = {}
    try:
        pods = v1.list_pod_for_all_namespaces(watch=False)
        for pod in pods.items:
            ns = pod.metadata.namespace
            pod_name = pod.metadata.name
            node_name = pod.spec.node_name or "æœªçŸ¥èŠ‚ç‚¹"
            pod_ip = pod.status.pod_ip or "æœªçŸ¥IP"
            # è·å–å®¹å™¨åˆ—è¡¨
            containers = [c.name for c in pod.spec.containers]
            
            # å…³é”®ä¿®å¤ï¼šå°†tuple keyè½¬ä¸ºå­—ç¬¦ä¸² "ns|pod_name"
            pod_key = f"{ns}|{pod_name}"
            pod_map[pod_key] = {
                "node_name": node_name,
                "pod_ip": pod_ip,
                "containers": containers
            }
        print(f"âœ… è·å–Podæ˜ å°„å…³ç³»ï¼ˆå…±{len(pod_map)}ä¸ªPodï¼‰")
        return pod_map
    except ApiException as e:
        print(f"âŒ æŸ¥è¯¢Podä¿¡æ¯å¤±è´¥ï¼š{e.reason}ï¼ˆ{e.status}ï¼‰")
        return {}

def get_k8s_pod_service_map():
    """è·å–Podâ†’Serviceçš„æ˜ å°„å…³ç³»ï¼š"ns|pod_name" â†’ [service1, service2, ...]"""
    v1 = client.CoreV1Api()
    pod_service_map = {}
    try:
        # å…ˆè·å–æ‰€æœ‰Service
        services = v1.list_service_for_all_namespaces(watch=False)
        for svc in services.items:
            ns = svc.metadata.namespace
            svc_name = svc.metadata.name
            # è·å–Serviceå…³è”çš„Podæ ‡ç­¾é€‰æ‹©å™¨
            selector = svc.spec.selector
            if not selector:
                continue
            
            # æ ¹æ®æ ‡ç­¾ç­›é€‰Pod
            label_selector = ",".join([f"{k}={v}" for k, v in selector.items()])
            pods = v1.list_pod_for_all_namespaces(watch=False, label_selector=label_selector)
            for pod in pods.items:
                # å…³é”®ä¿®å¤ï¼štuple â†’ string key
                pod_key = f"{pod.metadata.namespace}|{pod.metadata.name}"
                if pod_key not in pod_service_map:
                    pod_service_map[pod_key] = []
                if svc_name not in pod_service_map[pod_key]:
                    pod_service_map[pod_key].append(svc_name)
        
        # å¤„ç†æ— å…³è”Serviceçš„Pod
        all_pods = v1.list_pod_for_all_namespaces(watch=False)
        for pod in all_pods.items:
            pod_key = f"{pod.metadata.namespace}|{pod.metadata.name}"
            if pod_key not in pod_service_map:
                pod_service_map[pod_key] = ["æ— å…³è”Service"]
        
        print(f"âœ… è·å–Pod-Serviceæ˜ å°„å…³ç³»ï¼ˆå…±{len(pod_service_map)}ä¸ªPodï¼‰")
        return pod_service_map
    except ApiException as e:
        print(f"âŒ æŸ¥è¯¢Serviceä¿¡æ¯å¤±è´¥ï¼š{e.reason}ï¼ˆ{e.status}ï¼‰")
        return {}

def get_k8s_full_metadata():
    """è·å–K8så…¨é‡å…ƒæ•°æ®ï¼ˆèŠ‚ç‚¹/Pod/Serviceï¼‰"""
    global METADATA_MAP
    
    # ä¼˜å…ˆè¯»å–ç¼“å­˜
    if CACHE_CONTAINER_MAP and os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                METADATA_MAP = json.load(f)
                print(f"âœ… ä»ç¼“å­˜åŠ è½½K8så…ƒæ•°æ®ï¼ˆèŠ‚ç‚¹ï¼š{len(METADATA_MAP['node_map'])}ï¼ŒPodï¼š{len(METADATA_MAP['pod_map'])}ï¼‰")
                return METADATA_MAP
        except Exception as e:
            print(f"âš ï¸  è¯»å–ç¼“å­˜å¤±è´¥ï¼Œé‡æ–°æŸ¥è¯¢K8s APIï¼š{str(e)}")
    
    # ä»K8s APIæŸ¥è¯¢å…¨é‡å…ƒæ•°æ®
    load_k8s_config()
    METADATA_MAP["node_map"] = get_k8s_node_map()
    METADATA_MAP["pod_map"] = get_k8s_pod_map()
    METADATA_MAP["pod_service_map"] = get_k8s_pod_service_map()
    
    # å†™å…¥ç¼“å­˜ï¼ˆä¿®å¤åå¯æ­£å¸¸JSONåºåˆ—åŒ–ï¼‰
    if CACHE_CONTAINER_MAP:
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(METADATA_MAP, f, ensure_ascii=False, indent=2)
        print(f"âœ… K8så…ƒæ•°æ®å·²ç¼“å­˜åˆ°ï¼š{CACHE_FILE}")
    
    return METADATA_MAP

def get_node_info_by_instance(instance: str) -> tuple:
    """
    æ ¹æ®Prometheusçš„instanceï¼ˆå¦‚10.2.4.10:9100ï¼‰è·å–èŠ‚ç‚¹åç§°å’ŒIP
    è¿”å›ï¼š(node_name, node_ip)
    """
    # æå–IPï¼ˆå»æ‰ç«¯å£ï¼‰
    node_ip = instance.split(":")[0] if ":" in instance else instance
    
    # åå‘æŸ¥æ‰¾èŠ‚ç‚¹åç§°
    node_name = "æœªçŸ¥èŠ‚ç‚¹"
    for name, ip in METADATA_MAP["node_map"].items():
        if ip == node_ip:
            node_name = name
            break
    
    return node_name, node_ip

def get_pod_full_info(namespace: str, pod_name: str) -> dict:
    """
    è·å–Podçš„å…¨é‡ä¿¡æ¯
    è¿”å›ï¼š{node_name, node_ip, pod_ip, container_name, service_names}
    """
    # å…³é”®ä¿®å¤ï¼štuple â†’ string key
    pod_key = f"{namespace}|{pod_name}"
    # é»˜è®¤å€¼
    pod_info = {
        "node_name": "æœªçŸ¥èŠ‚ç‚¹",
        "node_ip": "æœªçŸ¥IP",
        "pod_ip": "æœªçŸ¥IP",
        "container_name": f"{pod_name}-æœªçŸ¥å®¹å™¨å®ä¾‹",
        "service_names": "æ— å…³è”Service"
    }
    
    # è·å–PodåŸºç¡€ä¿¡æ¯
    if pod_key in METADATA_MAP["pod_map"]:
        pod_meta = METADATA_MAP["pod_map"][pod_key]
        pod_info["node_name"] = pod_meta["node_name"]
        pod_info["pod_ip"] = pod_meta["pod_ip"]
        
        # è·å–èŠ‚ç‚¹IP
        if pod_meta["node_name"] in METADATA_MAP["node_map"]:
            pod_info["node_ip"] = METADATA_MAP["node_map"][pod_meta["node_name"]]
        
        # è·å–å®¹å™¨åç§°
        containers = pod_meta["containers"]
        if len(containers) == 1:
            pod_info["container_name"] = containers[0]
        elif len(containers) > 1:
            pod_info["container_name"] = ", ".join(containers)
    
    # è·å–å…³è”Service
    if pod_key in METADATA_MAP["pod_service_map"]:
        svc_names = METADATA_MAP["pod_service_map"][pod_key]
        pod_info["service_names"] = ", ".join(svc_names)
    
    return pod_info

def get_prometheus_data(query: str) -> list:
    """ä»Prometheusè·å–æ•°æ®"""
    end_ts = int(time.time())
    start_ts = end_ts - (TIME_RANGE_DAYS * 24 * 3600)
    
    params = {
        "query": query,
        "start": start_ts,
        "end": end_ts,
        "step": STEP
    }
    
    try:
        response = requests.get(PROMETHEUS_URL, params=params, timeout=60)
        response.raise_for_status()
        data = response.json()
        
        if data.get("status") != "success":
            print(f"âŒ PrometheusæŸ¥è¯¢å¤±è´¥ï¼š{data.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return []
        
        return data.get("data", {}).get("result", [])
    
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚Prometheuså¤±è´¥ï¼š{str(e)}")
        return []

def timestamp_to_readable(ts: str) -> str:
    """æ—¶é—´æˆ³è½¬å¯è¯»æ ¼å¼"""
    try:
        ts_int = int(float(ts))
        return datetime.fromtimestamp(ts_int).strftime("%Y-%m-%d %H:%M:%S")
    except (ValueError, TypeError):
        return "æ—¶é—´è½¬æ¢å¤±è´¥"

def export_to_csv(file_path: str, headers: list, data_rows: list):
    """å¯¼å‡ºæ•°æ®åˆ°CSV"""
    try:
        import csv
        with open(file_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(headers)
            writer.writerows(data_rows)
        print(f"âœ… å¯¼å‡ºå®Œæˆï¼š{file_path}")
    except Exception as e:
        print(f"âŒ å¯¼å‡ºCSVå¤±è´¥ {file_path}ï¼š{str(e)}")

# ===================== æ•°æ®å¯¼å‡ºå‡½æ•° =====================
def export_node_cpu():
    """å¯¼å‡ºèŠ‚ç‚¹CPUä½¿ç”¨ç‡ï¼ˆæ–°å¢èŠ‚ç‚¹åç§°ã€èŠ‚ç‚¹IPï¼‰"""
    print("\nğŸ”„ å¼€å§‹å¯¼å‡ºèŠ‚ç‚¹CPUä½¿ç”¨ç‡...")
    query = "100 - (avg by (instance) (irate(node_cpu_seconds_total{mode='idle'}[5m])) * 100)"
    data = get_prometheus_data(query)
    
    headers = [
        "Node Name(èŠ‚ç‚¹åç§°)",
        "Node IP(èŠ‚ç‚¹IP)",
        "Time(æ—¶é—´)(YYYY-MM-DD HH:MM:SS)",
        "CPU Usage(CPUä½¿ç”¨ç‡)(%)"
    ]
    
    data_rows = []
    for item in data:
        instance = item["metric"].get("instance", "æœªçŸ¥èŠ‚ç‚¹")
        node_name, node_ip = get_node_info_by_instance(instance)
        
        for value_pair in item["values"]:
            ts, cpu_value = value_pair
            readable_time = timestamp_to_readable(ts)
            data_rows.append([node_name, node_ip, readable_time, f"{cpu_value}%"])
    
    export_to_csv(f"{EXPORT_DIR}/node_cpu_7d.csv", headers, data_rows)

def export_node_mem():
    """å¯¼å‡ºèŠ‚ç‚¹å†…å­˜ä½¿ç”¨ç‡ï¼ˆæ–°å¢èŠ‚ç‚¹åç§°ã€èŠ‚ç‚¹IPï¼‰"""
    print("\nğŸ”„ å¼€å§‹å¯¼å‡ºèŠ‚ç‚¹å†…å­˜ä½¿ç”¨ç‡...")
    query = "(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100"
    data = get_prometheus_data(query)
    
    headers = [
        "Node Name(èŠ‚ç‚¹åç§°)",
        "Node IP(èŠ‚ç‚¹IP)",
        "Time(æ—¶é—´)(YYYY-MM-DD HH:MM:SS)",
        "Memory Usage(å†…å­˜ä½¿ç”¨ç‡)(%)"
    ]
    
    data_rows = []
    for item in data:
        instance = item["metric"].get("instance", "æœªçŸ¥èŠ‚ç‚¹")
        node_name, node_ip = get_node_info_by_instance(instance)
        
        for value_pair in item["values"]:
            ts, mem_value = value_pair
            readable_time = timestamp_to_readable(ts)
            data_rows.append([node_name, node_ip, readable_time, f"{mem_value}%"])
    
    export_to_csv(f"{EXPORT_DIR}/node_mem_7d.csv", headers, data_rows)

def export_node_network_in():
    """å¯¼å‡ºèŠ‚ç‚¹ç½‘ç»œå…¥æµé‡é€Ÿç‡ï¼ˆæ–°å¢èŠ‚ç‚¹åç§°ã€èŠ‚ç‚¹IPï¼‰"""
    print("\nğŸ”„ å¼€å§‹å¯¼å‡ºèŠ‚ç‚¹ç½‘ç»œå…¥æµé‡é€Ÿç‡...")
    query = "rate(node_network_receive_bytes_total[5m])"
    data = get_prometheus_data(query)
    
    headers = [
        "Node Name(èŠ‚ç‚¹åç§°)",
        "Node IP(èŠ‚ç‚¹IP)",
        "Network Card(ç½‘å¡)",
        "Time(æ—¶é—´)(YYYY-MM-DD HH:MM:SS)",
        "Inbound Traffic Rate(å…¥æµé‡é€Ÿç‡)(B/s)"
    ]
    
    data_rows = []
    for item in data:
        instance = item["metric"].get("instance", "æœªçŸ¥èŠ‚ç‚¹")
        node_name, node_ip = get_node_info_by_instance(instance)
        device = item["metric"].get("device", "æœªçŸ¥ç½‘å¡")
        
        for value_pair in item["values"]:
            ts, in_value = value_pair
            readable_time = timestamp_to_readable(ts)
            data_rows.append([node_name, node_ip, device, readable_time, f"{in_value} B/s"])
    
    export_to_csv(f"{EXPORT_DIR}/node_network_in_7d.csv", headers, data_rows)

def export_node_network_out():
    """å¯¼å‡ºèŠ‚ç‚¹ç½‘ç»œå‡ºæµé‡é€Ÿç‡ï¼ˆæ–°å¢èŠ‚ç‚¹åç§°ã€èŠ‚ç‚¹IPï¼‰"""
    print("\nğŸ”„ å¼€å§‹å¯¼å‡ºèŠ‚ç‚¹ç½‘ç»œå‡ºæµé‡é€Ÿç‡...")
    query = "rate(node_network_transmit_bytes_total[5m])"
    data = get_prometheus_data(query)
    
    headers = [
        "Node Name(èŠ‚ç‚¹åç§°)",
        "Node IP(èŠ‚ç‚¹IP)",
        "Network Card(ç½‘å¡)",
        "Time(æ—¶é—´)(YYYY-MM-DD HH:MM:SS)",
        "Outbound Traffic Rate(å‡ºæµé‡é€Ÿç‡)(B/s)"
    ]
    
    data_rows = []
    for item in data:
        instance = item["metric"].get("instance", "æœªçŸ¥èŠ‚ç‚¹")
        node_name, node_ip = get_node_info_by_instance(instance)
        device = item["metric"].get("device", "æœªçŸ¥ç½‘å¡")
        
        for value_pair in item["values"]:
            ts, out_value = value_pair
            readable_time = timestamp_to_readable(ts)
            data_rows.append([node_name, node_ip, device, readable_time, f"{out_value} B/s"])
    
    export_to_csv(f"{EXPORT_DIR}/node_network_out_7d.csv", headers, data_rows)

def export_pod_cpu():
    """å¯¼å‡ºPod CPUä½¿ç”¨ç‡ï¼ˆæ–°å¢èŠ‚ç‚¹åç§°/IPã€Pod IPã€Serviceã€å®¹å™¨å®ä¾‹åç§°ï¼‰"""
    print("\nğŸ”„ å¼€å§‹å¯¼å‡ºPod CPUä½¿ç”¨ç‡...")
    query = "rate(container_cpu_usage_seconds_total{image!=''}[5m])"
    data = get_prometheus_data(query)
    
    headers = [
        "Namespace(å‘½åç©ºé—´)",
        "Pod Name(Podåç§°)",
        "Pod IP(Podåœ°å€)",
        "Node Name(èŠ‚ç‚¹åç§°)",
        "Node IP(èŠ‚ç‚¹IP)",
        "å…³è”Service(Serviceåç§°)",
        "Container Name(å®¹å™¨å®ä¾‹åç§°)",
        "Time(æ—¶é—´)(YYYY-MM-DD HH:MM:SS)",
        "CPU Usage(CPUä½¿ç”¨ç‡)(Core)"
    ]
    
    data_rows = []
    for item in data:
        namespace = item["metric"].get("namespace", "æœªçŸ¥å‘½åç©ºé—´")
        pod_name = item["metric"].get("pod", "æœªçŸ¥Pod")
        
        # è·å–Podå…¨é‡ä¿¡æ¯
        pod_info = get_pod_full_info(namespace, pod_name)
        
        for value_pair in item["values"]:
            ts, cpu_value = value_pair
            readable_time = timestamp_to_readable(ts)
            data_rows.append([
                namespace,
                pod_name,
                pod_info["pod_ip"],
                pod_info["node_name"],
                pod_info["node_ip"],
                pod_info["service_names"],
                pod_info["container_name"],
                readable_time,
                f"{cpu_value} Core"
            ])
    
    export_to_csv(f"{EXPORT_DIR}/pod_cpu_7d.csv", headers, data_rows)

def export_pod_mem():
    """å¯¼å‡ºPodå†…å­˜ä½¿ç”¨é‡ï¼ˆæ–°å¢èŠ‚ç‚¹åç§°/IPã€Pod IPã€Serviceã€å®¹å™¨å®ä¾‹åç§°ï¼‰"""
    print("\nğŸ”„ å¼€å§‹å¯¼å‡ºPodå†…å­˜ä½¿ç”¨é‡...")
    query = "container_memory_usage_bytes{image!=''}"
    data = get_prometheus_data(query)
    
    headers = [
        "Namespace(å‘½åç©ºé—´)",
        "Pod Name(Podåç§°)",
        "Pod IP(Podåœ°å€)",
        "Node Name(èŠ‚ç‚¹åç§°)",
        "Node IP(èŠ‚ç‚¹IP)",
        "å…³è”Service(Serviceåç§°)",
        "Container Name(å®¹å™¨å®ä¾‹åç§°)",
        "Time(æ—¶é—´)(YYYY-MM-DD HH:MM:SS)",
        "Memory Usage(å†…å­˜ä½¿ç”¨é‡)(GB)"
    ]
    
    data_rows = []
    for item in data:
        namespace = item["metric"].get("namespace", "æœªçŸ¥å‘½åç©ºé—´")
        pod_name = item["metric"].get("pod", "æœªçŸ¥Pod")
        
        # è·å–Podå…¨é‡ä¿¡æ¯
        pod_info = get_pod_full_info(namespace, pod_name)
        
        for value_pair in item["values"]:
            ts, mem_value = value_pair
            readable_time = timestamp_to_readable(ts)
            # è½¬æ¢ä¸ºGBï¼ˆä¿ç•™2ä½å°æ•°ï¼‰
            try:
                mem_gb = round(float(mem_value) / 1024 / 1024 / 1024, 2)
                mem_str = f"{mem_gb} GB"
            except ValueError:
                mem_str = f"{mem_value} Byte"
            
            data_rows.append([
                namespace,
                pod_name,
                pod_info["pod_ip"],
                pod_info["node_name"],
                pod_info["node_ip"],
                pod_info["service_names"],
                pod_info["container_name"],
                readable_time,
                mem_str
            ])
    
    export_to_csv(f"{EXPORT_DIR}/pod_mem_7d.csv", headers, data_rows)

def export_pod_network_in():
    """å¯¼å‡ºPodç½‘ç»œå…¥æµé‡é€Ÿç‡ï¼ˆæ–°å¢èŠ‚ç‚¹åç§°/IPã€Pod IPã€Serviceã€å®¹å™¨å®ä¾‹åç§°ï¼‰"""
    print("\nğŸ”„ å¼€å§‹å¯¼å‡ºPodç½‘ç»œå…¥æµé‡é€Ÿç‡...")
    query = "rate(container_network_receive_bytes_total{image!=''}[5m])"
    data = get_prometheus_data(query)
    
    headers = [
        "Namespace(å‘½åç©ºé—´)",
        "Pod Name(Podåç§°)",
        "Pod IP(Podåœ°å€)",
        "Node Name(èŠ‚ç‚¹åç§°)",
        "Node IP(èŠ‚ç‚¹IP)",
        "å…³è”Service(Serviceåç§°)",
        "Container Name(å®¹å™¨å®ä¾‹åç§°)",
        "Time(æ—¶é—´)(YYYY-MM-DD HH:MM:SS)",
        "Inbound Traffic Rate(å…¥æµé‡é€Ÿç‡)(B/s)"
    ]
    
    data_rows = []
    for item in data:
        namespace = item["metric"].get("namespace", "æœªçŸ¥å‘½åç©ºé—´")
        pod_name = item["metric"].get("pod", "æœªçŸ¥Pod")
        
        # è·å–Podå…¨é‡ä¿¡æ¯
        pod_info = get_pod_full_info(namespace, pod_name)
        
        for value_pair in item["values"]:
            ts, in_value = value_pair
            readable_time = timestamp_to_readable(ts)
            data_rows.append([
                namespace,
                pod_name,
                pod_info["pod_ip"],
                pod_info["node_name"],
                pod_info["node_ip"],
                pod_info["service_names"],
                pod_info["container_name"],
                readable_time,
                f"{in_value} B/s"
            ])
    
    export_to_csv(f"{EXPORT_DIR}/pod_network_in_7d.csv", headers, data_rows)

def export_pod_network_out():
    """å¯¼å‡ºPodç½‘ç»œå‡ºæµé‡é€Ÿç‡ï¼ˆæ–°å¢èŠ‚ç‚¹åç§°/IPã€Pod IPã€Serviceã€å®¹å™¨å®ä¾‹åç§°ï¼‰"""
    print("\nğŸ”„ å¼€å§‹å¯¼å‡ºPodç½‘ç»œå‡ºæµé‡é€Ÿç‡...")
    query = "rate(container_network_transmit_bytes_total{image!=''}[5m])"
    data = get_prometheus_data(query)
    
    headers = [
        "Namespace(å‘½åç©ºé—´)",
        "Pod Name(Podåç§°)",
        "Pod IP(Podåœ°å€)",
        "Node Name(èŠ‚ç‚¹åç§°)",
        "Node IP(èŠ‚ç‚¹IP)",
        "å…³è”Service(Serviceåç§°)",
        "Container Name(å®¹å™¨å®ä¾‹åç§°)",
        "Time(æ—¶é—´)(YYYY-MM-DD HH:MM:SS)",
        "Outbound Traffic Rate(å‡ºæµé‡é€Ÿç‡)(B/s)"
    ]
    
    data_rows = []
    for item in data:
        namespace = item["metric"].get("namespace", "æœªçŸ¥å‘½åç©ºé—´")
        pod_name = item["metric"].get("pod", "æœªçŸ¥Pod")
        
        # è·å–Podå…¨é‡ä¿¡æ¯
        pod_info = get_pod_full_info(namespace, pod_name)
        
        for value_pair in item["values"]:
            ts, out_value = value_pair
            readable_time = timestamp_to_readable(ts)
            data_rows.append([
                namespace,
                pod_name,
                pod_info["pod_ip"],
                pod_info["node_name"],
                pod_info["node_ip"],
                pod_info["service_names"],
                pod_info["container_name"],
                readable_time,
                f"{out_value} B/s"
            ])
    
    export_to_csv(f"{EXPORT_DIR}/pod_network_out_7d.csv", headers, data_rows)

# ===================== ä¸»å‡½æ•° =====================
def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¯¼å‡ºK8sé›†ç¾¤è¿‘{}å¤©ç›‘æ§æ•°æ®ï¼ˆæœ€ç»ˆä¿®å¤ç‰ˆï¼‰...".format(TIME_RANGE_DAYS))
    
    # 1. åˆ›å»ºå¯¼å‡ºç›®å½•
    create_export_dir()
    
    # 2. é¢„åŠ è½½K8så…¨é‡å…ƒæ•°æ®ï¼ˆæ ¸å¿ƒï¼‰
    print("\nğŸ”„ åŠ è½½K8så…¨é‡å…ƒæ•°æ®ï¼ˆèŠ‚ç‚¹/Pod/Serviceï¼‰...")
    get_k8s_full_metadata()
    
    # 3. æ‰§è¡Œå¯¼å‡ºä»»åŠ¡
    export_node_cpu()
    export_node_mem()
    export_node_network_in()
    export_node_network_out()
    export_pod_cpu()
    export_pod_mem()
    export_pod_network_in()
    export_pod_network_out()
    
    # 4. è¾“å‡ºæ€»ç»“
    print("\n========================================")
    print("ğŸ‰ æ‰€æœ‰ç›‘æ§æ•°æ®å¯¼å‡ºå®Œæˆï¼")
    print(f"ğŸ“ å¯¼å‡ºç›®å½•ï¼š{os.path.abspath(EXPORT_DIR)}")
    print("ğŸ“‹ å¯¼å‡ºæ–‡ä»¶åˆ—è¡¨ï¼š")
    for file in os.listdir(EXPORT_DIR):
        print(f"   - {file}")
    print("========================================")

if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–
    required_pkgs = ["requests", "kubernetes"]
    missing_pkgs = []
    
    for pkg in required_pkgs:
        try:
            __import__(pkg)
        except ImportError:
            missing_pkgs.append(pkg)
    
    if missing_pkgs:
        print(f"âŒ ç¼ºå°‘ä¾èµ–åŒ…ï¼š{', '.join(missing_pkgs)}")
        print(f"ğŸ“¦ æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼špip3 install {' '.join(missing_pkgs)}")
        exit(1)
    
    # å¯åŠ¨ä¸»æµç¨‹
    main()
```

